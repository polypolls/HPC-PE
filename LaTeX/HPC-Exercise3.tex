\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{listings}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{hyperref}

\title{\textbf{High Performance Computer Architectures Practical Course \\ - Exercise 3 -} \\[10mm]}
\author{Tutorium 1 \\[10mm] David Jordan (6260776) \\[1mm] Florian RÃ¼ffer (7454628) \\[1mm] Michael Samjatin (7485765) \\[10mm]}


\lstset{
    language=C++,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    numbers=left,
    numberstyle=\normalsize,
    breaklines=true,
    showstringspaces=false,
    frame=single,
    linewidth=1\linewidth,
    captionpos=b
}
\renewcommand{\lstlistingname}{File}% Listing -> Algorithm
\renewcommand{\lstlistlistingname}{List of \lstlistingname s}% List of Listings -> List of Algorithms

\begin{document}
\maketitle
\newpage
\section{Neural Networks and SIMD}
% To add numbered subsection / section / etc. remove "*".
\subsection*{1}
\subsection*{2}
The task for this exercise is to add SIMD (Single Instruction, Multiple Data) support to our neural network functions.
We will start of with the simd-ized version of the ReLU activation function. \\

\noindent The first noticable detail is the changed input. We input type is "fvec".
Fvec is a SIMD-packed set of values, to be exact 4 packed 32-bit floating point numbers.
Once again we create a result vector, the same size as the input. Subsequently we loop for 1.) every element of fvec (here this is 4) and 2.)
the number of elements of the input. As a result, we loop over every element of the input. \\

\noindent Finally, we check the conditions associated with the ReLU activation function for every of those elements.
The defintion of ReLU is provided in the Appendix (\hyperref[subsec:ReLU]{{\ref*{subsec:ReLU}}})


\begin{lstlisting}[caption=SIMD-ized ReLU]
    std::vector<fvec> MLPMath::applyReLU(std::vector<fvec>& input) {

    std::vector<fvec> result(input.size());

    for (std::size_t iv = 0; iv < fvecLen; iv++) {
        for (std::size_t i = 0; i < input.size(); i++) {
            if (input[i][iv] > 0.0f) {
                result[i][iv] = input[i][iv];
            }
            else {
                result[i][iv] = 0.0f;
            }
        }
    }
    
    return result;
}
\end{lstlisting}

\newpage
\noindent Another essential part is the backPropActivation function.
Here we provide just the code snippet, which specifically implements ReLU. \\

\noindent In this case there are not many significant changes.
We want to ensure, that this function will execute properly if provided with a fvec vector input.
For this to work, line 4 is the most essential one to understand. Here the fvec-type overloads the "$>$"-operator.
To understand the underlying functionality, please view the following example. \\

\noindent "rawOutput" contains the elements: [1.0, -1.0, 2.0, -0.5] \\
"zero" contains the elements [0.0, 0.0, 0.0, 0.0] \\

\noindent Overloading will produce a result like this:
[$1.0>0.0,-1.0>0.0,2.0>0.0,-0.5>0$] \\\\
\noindent This checks the condition for every element and can the abbreviated into the following form: [0xFFFFFFFF, 0x00000000, 0xFFFFFFFF, 0x00000000]
(1 - True \& 0 - False)

\begin{lstlisting}[caption=backPropActivation]
    case 1:{
        fvec zero = fvec(0.0f);
        for (std::size_t i = 0; i < rDelta.size(); i++) {
            rDelta[i] = (rawOutput[i] > zero) * rDelta[i];
        }
        break;
    }
\end{lstlisting}

\newpage

\noindent Finally we need to incorporate our SIMD-ized ReLU function into the applyActivation function (Line 10-14).
\begin{lstlisting}[caption=applyActivation]
    std::vector<fvec> MLPNet::applyActivation(std::vector<fvec>& input) {

    std::vector<fvec> output(input.size());
    switch (activationType_) {
        case 0: {// TanH
            output = MLPMath::applyTanH(input);
            return output;
            break;
        }
        case 1: {// SIMD-ized ReLU
            output = MLPMath::applyReLU(input);
            return output;
            break;
        }
        default: {
            return output;
            break;
        }
    }
}
\end{lstlisting}


\subsection*{3}

\section{Matrix}
In this task we need to speed up Matrix calculations using SIMD.
As displayed below, we want to go from matrix a to matrix c.

\[
a = \begin{bmatrix}
a_0 & a_1 & a_2 \\
a_3 & a_4 & a_5 \\
a_6 & a_7 & a_8
\end{bmatrix}
\]

\[
c = \begin{bmatrix}
\sqrt{a_0} & \sqrt{a_1} & \sqrt{a_2} \\
\sqrt{a_3} & \sqrt{a_4} & \sqrt{a_5} \\
\sqrt{a_6} & \sqrt{a_7} & \sqrt{a_8}
\end{bmatrix}
\]
\newpage

\noindent First of all we need to change the input \& output matrices (output for scalar computation stays the same).
We do this by changing the memory alignment of the values of the matrices.
This is a requirement for SIMD-ized operations.
\begin{lstlisting}[caption=Matrix.cpp]
float a[N][N] __attribute__((aligned(16)));      // input array
float c[N][N]; // output array for scalar computations
float c_simd[N][N] __attribute__((aligned(16))); // output array for SIMD computations
\end{lstlisting}

\noindent The loops for the computation part remain mostly unchanged to the scalar version.
The inner loop runs N-times but our loop variable increases by 4, because each vector has 4 elements and those can the calculated in parallel.
We loop for $NIter - 1$ times, to neglect memory reading time.
Subsequently we loop over matrix a to transform it into matrix c (NxN for rows and columns).
The distinct section ranges from line 5 to line 7. We define two new vectors, called aVec and cVec, which suppose to be the vector counter parts to the scalar versions of matrix a and matrix c.
At the beginning those were initialized with float values, so we use the reinterpret\_cast command to allow treating
a variables memory representation as another type. In this case we substitute float for fvec.
Finally we just plug those values into the template function to calculate the root (Line 7).

\begin{lstlisting}[caption=Matrix.cpp]
TStopwatch timerSIMD;
for( int ii = 0; ii < NIter; ii++ )
    for( int i = 0; i < N; i++ ) {
        for( int j = 0; j < N; j++ ) {
            fvec &aVec = reinterpret_cast<fvec&>(a[i][j]);
            fvec &cVec = reinterpret_cast<fvec&>(c_simd[i][j]);
            cVec = f(aVec);
        }
    }
timerSIMD.Stop();
\end{lstlisting}

\noindent We can compute 4 values in parallel and therefore can expect a speed-up factor of 4. Due to running environment deviation, the speed-up factor varies.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{matrix-output.png} 
    \caption{Output}
    \label{fig:example}
  \end{figure}
\section{Quadratic Equation}
\section{CheckSum}

\section{Appendix}
\subsection{ReLU}\label{subsec:ReLU}
\begin{center}
    \begin{tikzpicture}
    \begin{axis}[
        xlabel={$x$},
        ylabel={$y$},
        axis lines=middle,
        xmin=-5, xmax=5,
        ymin=-0.5, ymax=5,
        xtick={-5,-4,-3,-2,-1,0,1,2,3,4,5},
        ytick={0,1,2,3,4,5},
        yticklabels={0,1,2,3,4,5},
        xticklabels={-5,-4,-3,-2,-1,0,1,2,3,4,5},
        samples=100,
        domain=-5:5,
        smooth,
        thick
    ]
    \addplot+[mark=none] {max(0,x)};
    \end{axis}
    \end{tikzpicture}
    \end{center}
    \[
    \text{ReLU}(x) =
    \begin{cases}
    x, & \text{if } x \geq 0 \\
    0, & \text{otherwise}
    \end{cases}
    \]



\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{filler.png} 
  \caption{Add caption}
  \label{fig:example}
\end{figure}


\end{document}
